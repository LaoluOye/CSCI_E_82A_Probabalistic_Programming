{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Q Learning\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "## Stephen Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To date we have only considered reinforcement learning problems with a small number of **discrete states**. So far, we have applied **tabular algorithms** to the solution of RL problems. However, tabular methods will not work in cases where the tables are too large to be held in computer memory or where states are not discrete values. \n",
    "\n",
    "But, many practical problems have eight very large numbers of discrete states or have continuous states. Some example of such problems:  \n",
    "1. A Game such as chess or backgammon with a great number of possible board states. While the number of states is **countably finite** there are far too many for tabular solutions. For example, consider that each piece in a chess game can occupy any of 64 positions, and that there are anywhere between 32 pieces and 2 pieces on the board at a given time step. This situation leads to an explosion in possible states. Other games, such a Go, have far more states than chess.   \n",
    "2. A simple flight control system for a drone which has an infinite number of states in terms of 3-dimensional velocity, acceleration, and position, along with pitch yaw and role. All 12 of these variables have **continuous values** and, thus, an **infinite number of states**.   \n",
    "\n",
    "To address such problems we must have a better representations. In this lesson we will focus on a powerful class of representations known as **function approximation**. By using function approximation we can represent a large number of states, even an infinite number, with a limited number of **parameters**. Using function approximation problems of great complexity can be tackled, at least in theory.  \n",
    "\n",
    "A reinforcement learning agent using function approximation is illustrated schematically in the figure below. \n",
    "\n",
    "<img src=\"img/AgentEnvironment.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Reinforcement Learning Agent with Function Approximation Representation and Environment** </center>  \n",
    "\n",
    "In this lesson we will only address function approximation methods for **episodic** tasks. \n",
    "\n",
    ">**Suggested reading:** The following sections of Sutton and Barto, second edition, provide additional scope for the topics of this lesson: 9.1, 9.2, 9.3, 9.4, 9.5, 10.1, and 10.2.\n",
    "\n",
    ">**For further exploration:** The resources of [Open AI Gym](https://gym.openai.com/) provide a rich set of simulators along with a framework to help you construct and test RL algorithms. \n",
    "\n",
    ">**Code examples in depth:** *Deep Reinforcement Learning Hands-On* by Maxim Lapan, Packt, 2018, provides considerable detail on deep Q learning. The book contains in-depth examples and discusses important implementation details. The examples in this book use the PyTorch deep learning framework. \n",
    "\n",
    "In order to run all of the examples in this notebook you will need to install [Keras](https://keras.io/) and h5py; `pip install h5py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation for Non-Tabular RL\n",
    "\n",
    "The main question we will address in this lesson is what representations can we use when tabular methods will not work. The idea key idea is to **encode** a large, possibly infinite, number of states with a **few parameters**. Thus, **state values** or **action values** are represented by some **function of the state** of the environment.  \n",
    "\n",
    "There are, in fact, many possible approaches to this problem. Here we will focus on function approximation methods of which a few examples are: \n",
    "1. Simple **linear** and **polynomial** representations,\n",
    "2. **Fourier basis function** representations,\n",
    "3. **Course coding** using overlapping circular or elliptical regions,\n",
    "4. Various forms of **tile coding**,\n",
    "5. **Radial basis functions** or kernels,\n",
    "5. **Fully connected** deep neural networks, \n",
    "6. **Convolutional** deep neural networks. \n",
    "\n",
    "Each of the first five methods involve **coding** using some type of **basis function**. Basis functions rely on an implicit assumption that **nearby states have similar values**.  The representation is then **linear in the parameters** and **linear solution methods** are used to find these parameters. Linear methods have the advantage of computational efficiency. Further, at least for **on-policy** algorithms, **convergence is guaranteed**.     \n",
    "\n",
    "The deep neural networks provide a distinctly nonlinear function approximation method. Typically, deep neural networks are used for **off-policy Q-Learning** methods. The convergence properties of these algorithms can be problematic. In fact, there are few guarantees of convergence with **off-policy function approximation** algorithms.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile coding\n",
    "\n",
    "Tile coding is a flexible and expressive function approximation method. The basic idea is simple. The **state space is divided** into small patches using a regular pattern of **geometric shapes** or **tiles**. The function approximation has one parameter (weight) for each tile.    \n",
    "\n",
    "An example tile coding of a two dimensional state space is shown in the figure below. In this case, a uniform 8x8 set of tiles are used, leading to a representation with 64 parameters. \n",
    "\n",
    "<img src=\"img/Tile1.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **Two-dimensional state space encoded by 8x8 rectangular tiles** </center>\n",
    "\n",
    "In the above diagram the states shown by **X** are in the same tile and will be coded with the same parameter. The states show by **O** are in different tiles and are coded with different parameters. \n",
    "\n",
    "However, the tile codings are far from unique. Consider the tiling of the same state space shown in the figure below. In this case the tiles are a 4X16 grid. As in the first case, there are still 64 parameters. \n",
    "\n",
    "<img src=\"img/Tile2.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **Two-dimensional state space encoded by 4x16 rectangular tiles** </center>\n",
    "\n",
    "In the above coding the states shown with **O** are in the same tile and represented by the same parameter. But, the states shown as **X** are now in different tiles and are represented by different parameters. \n",
    "\n",
    "A great many tile coding schemes are possible. Commonly, multiple tiling schemes are used simultaneously. This practice allows for the capture of information at **different scales**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep neural networks\n",
    "\n",
    "Deep neural networks are powerful function approximation methods. In most cases one of two deep neural network architectures are used:\n",
    "1. Fully connected network are used for cases where there are complex and highly nonlinear relationships between state values or action values. \n",
    "2. Convolutional networks are useful for cases where there is value coherency between adjacent states. Common examples include images and time series data.  \n",
    "\n",
    "In principle deep neural networks can approximate even highly complex nonlinear functions. Neural networks have been reviewed in a previous lesson. Here, we will just summarize some of the drawbacks of this attractive representation. \n",
    "1. Given the large number of parameters a large number of episodes are required for training. It is not unusual for several tens of millions of episodes to be required. Therefore, training time can be significantly longer than for other algorithms.\n",
    "2. As a result of the large number of parameters (high degrees of freedom) over-fitting is a constant problem. Careful attention must be paid to regularization methods. \n",
    "3. Deep neural network are known to have **brittle behavior**, wherein small changes in the input can result in surprising or unexpected predictions.\n",
    "\n",
    "The foregoing not withstanding, trained neural network models can be reasonably computationally efficient. In fact, prediction using trained neural network models is preformed routinely, even in embedded environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Function and Stochastic Gradient Decent\n",
    "\n",
    "Following the discussion above, basis function methods are linear in their parameters. In the following we will express these parameters as a vector of model weights, $\\mathbf{w}$. We can then represent the approximate state value function with state $x(s)$ as:\n",
    "\n",
    "$$\\hat{v}(s,\\mathbf{w}) = \\mathbf{w}^T \\mathbf{x}(s) =  \\sum_{i=1}^d   w_i  x_i(s)$$\n",
    "\n",
    "In principle, **stochastic gradient decent** algorithms are an efficient way to solve for the weights, $\\mathbf{w}$. At each step the update is just:\n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha\\ E_{\\hat{p}data}\\Big[ \\nabla_{w} J(\\mathbf{w}_t) \\Big]\\\\\n",
    "= \\mathbf{w}_t + \\alpha  \\big[v_{\\pi}(s) - \\hat{v}(S_t, \\mathbf{w}_t) \\big]\\nabla_w \\hat{v}(S_t,\\mathbf{w}_t)$$\n",
    "\n",
    "where, $\\hat{p}data$ is the mini-batch, and gradient is given by:   \n",
    "\n",
    "$$\\hat\\nabla_w {v}(S_t,\\mathbf{w}_t) = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\hat{v}(S_t,\\mathbf{w}_t)}{\\partial w_1} \\\\\n",
    "\\frac{\\partial \\hat{v}(S_t,\\mathbf{w}_t)}{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\hat{v}(S_t,\\mathbf{w}_t)}{\\partial w_d}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "In many practical algorithms, $\\hat{v}(s)$ is a **bootstrapped** approximation. This means are error term and gradient are not exact. We call such algorithms **semi-gradient decent** methods as they use an approximation of the gradient. This approach generally works well, but does not have the strong convergence guarantees of stochastic gradient decent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximation Error\n",
    "\n",
    "As already noted, when tabular algorithms are not feasible, we must resort to function approximation. We can use function approximation for both state values, $\\hat{v}(s)$, and action values, $\\hat{q}(s,a)$. We should always keep in mind that we are dealing with **approximations** and will never know the true state values, $v_{\\pi}(s)$, and action values, $q_{\\pi}(s,a)$. There will always be some **error** between the true values and the approximated values. For example, for state value approximation we can express this error as the **mean squared value error**:\n",
    "\n",
    "$$\\overline{VE}(w) = \\sum_{s \\in S} \\mu(s) \\Big[ v_{\\pi}(s) -  \\hat{v}(s,\\mathbf{w}) \\Big]^2$$  \n",
    "\n",
    "Here, $\\mu(s)$ is a weight indicating home important the state $s$ is. For on-policy RL, $\\mu(s)$ is a probability known as the **on-policy distribution**. \n",
    "\n",
    "A similar error metric can be constructed for $\\hat{q}(s,a)$.  \n",
    "\n",
    "In practical terms there is a trade-off between the complexity of the approximate representation and the error. More complex representations require more parameters, but have lower error and vice versa. This situation is exactly analogous to the bias-variance trade-off, familiar from machine learning.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mountain Car Problem\n",
    "\n",
    "The [mountain car problem](https://en.wikipedia.org/wiki/Mountain_car_problem) was first proposed in the [Andew Moore's Ph.D. dissertation (1990)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.2654). The mountain car problem has become a canonical testbed for many reinforcement learning algorithms. \n",
    "\n",
    "In this problem an under-powered car must climb a steep hill. However, the car does not have sufficient engine power to climb the grade. The car must travel up another hill in order to gain sufficient speed (actually kinetic energy) to climb the large hill. \n",
    "\n",
    "The position, $x$, and velocity, $\\dot{x}$, of the car are the state variables. The updates of the state variables at each time step are determined by the following equations: \n",
    "\n",
    "$$x' = x + \\dot{x} \\\\\n",
    "\\dot{x}' = \\dot{x} + 0.001 * \\ddot{x} - 0.0025 * cos(3 * x)$$\n",
    "\n",
    "The object of this problem is to find the optimal acceleration given the car state to allow the car to get to the top of the hill. The car has three acceleration states, $\\ddot{x}$, which must be selected by the agent:\n",
    "\n",
    "$$\\ddot{x} = \\{ -1.0, 0.0, 1.0 \\}$$\n",
    "\n",
    "The position and velocity are bounded, with the goal at the upper bound of position:\n",
    "\n",
    "$$-1.2 \\le x \\le 0.5 \\\\\n",
    "-0.07 \\le \\dot{x} \\le 0.07$$\n",
    "\n",
    "The reward at each time step is -1.0 and the reward for reaching the goal is 100.  \n",
    "\n",
    "The car is randomly initialized using a uniform distribution:\n",
    "\n",
    "$$p(x_0) = uniform(-0.6 \\le x_0 \\le -0.4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mountain car problem is hard\n",
    "\n",
    "At first glance, the mountain car problem may seem like it should have an easy solution. However, looks are deceptive. The learning an optimal policy for this problem is difficult. In fact, conventional control theory approaches fail to provide solutions. Some reasons for this difficulty include:\n",
    "1. The non-linear coupling between the two state variables, which makes the state transitions between the infinite number of states hard to predict.\n",
    "2. The delayed reward which is only observed when the goal is achieved. This fact is common to many difficult RL problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation of car environment\n",
    "\n",
    "The code in the cell below **simulates the car environment**. Two functions are used by the agent to interact with the environment:\n",
    "1. The `sim_car` function returns a state transition and a reward, given the agent's current state and an action. In addition, a flag is returned to indicate if the goal has been reached.\n",
    "2. The `initialize_car` function returns a random starting position for the car within the specified bounds. \n",
    "\n",
    "Taken together, calls to these two functions define the **boundary between the agent and the environement**. Execute the code in the cell below to exercise these functions and examine the resulting plots for a case where the acceleration is set to 0. \n",
    "\n",
    ">**Note:** An Open AI Gym [environment simulator](https://gym.openai.com/envs/MountainCarContinuous-v0/) for the mountain car problem is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import cos\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sim_car(x, x_dot, acceleration, x_lims = (-1.2,0.5), x_dot_lims = (-0.07,0.07)):\n",
    "    ## Compute velocity within limits\n",
    "    x_dot_prime = x_dot + 0.001 * acceleration - 0.0025 * cos(3 * x)\n",
    "    if(x_dot_prime < x_dot_lims[0]): x_dot_prime = x_dot_lims[0]\n",
    "    if(x_dot_prime > x_dot_lims[1]): x_dot_prime = x_dot_lims[1]\n",
    "        \n",
    "    ## Now update position\n",
    "    x_prime = x + x_dot_prime\n",
    "    if(x_prime < x_lims[0]): x_prime = x_lims[0]\n",
    "    if(x_prime > x_lims[1]): x_prime = x_lims[1]\n",
    "      \n",
    "    ## At the terminal state or not and set reward\n",
    "    if(x_prime >= x_lims[1]): \n",
    "        done = True\n",
    "        reward = 100.0\n",
    "    else: \n",
    "        done = False\n",
    "        reward = -1.0\n",
    "        \n",
    "    return(x_prime, x_dot_prime, done, reward)    \n",
    "        \n",
    "def initalize_car(x_lims = (-0.6,-0.4)):\n",
    "    ## Find random start for car\n",
    "    return(nr.uniform(x_lims[0],x_lims[1]))\n",
    "\n",
    "## Test the function\n",
    "a = -0.0\n",
    "x_dot = [0.0]\n",
    "x = [initalize_car()]\n",
    "for i in range(100):\n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a)\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "def plot_car(x, x_dot):    \n",
    "    ## Plot car position\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(211)    \n",
    "    ax1.plot(x)\n",
    "    ax1.set_ylabel('Positon of car')\n",
    "    \n",
    "    ## PLot car velocity\n",
    "    ax2 = fig.add_subplot(212)  \n",
    "    ax2.plot(x_dot)\n",
    "    ax2.set_ylabel('Velocity of car')\n",
    "    ax2.set_xlabel('Time')\n",
    "    \n",
    "plot_car(x,x_dot)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no acceleration applied, the car oscillates back and forth. The motion is not damped since the simulator includes no friction term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep NN Function Approximation for the Mountain Car Problem\n",
    "\n",
    "A general, but computationally intensive, solution to the mountain car problem can be obtained using **deep Q-Learning**. Here we use a neural network model as a function approximator for the action values, $\\hat{q}(S_{t}, A_{t}, w_{t-1})$.   \n",
    "\n",
    "The code in the cell below defines a simple neural network model to approximate the action values for the mountain car problems. A few key points include:\n",
    "- There are three input variables, the two state variables, position and velocity, along with the action. \n",
    "- A single hidden layer of 20 units is used. This is a rather spare representation, but is chosen for convenience of the demonstration rather than accuracy. \n",
    "- Since over-fitting is a constant problem with neural networks three regularization methods are applied, l2 regularization, dropout regularization and early stopping. \n",
    "- Since this is a regression problem, to approximate the numeric action value, the output layer consists of a single unit with linear activation. \n",
    "\n",
    "Execute this code to run the simple test case.\n",
    "\n",
    ">**Note:** The code shown in this notebook is intended to illustrate the basic concepts of deep Q-Learning. For more robust solutions you are advised to look at the contributed [keras-rl package](https://github.com/keras-rl/keras-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.0002168]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import keras.utils.np_utils as ku\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "def DL_model():\n",
    "    ## Define the Keras model\n",
    "    function_approx = models.Sequential()\n",
    "    function_approx.add(layers.Dense(3, activation = 'relu', input_shape = (3,), \n",
    "                                     kernel_regularizer=regularizers.l2(0.01)))\n",
    "    function_approx.add(Dropout(0.5)) # Use 50% dropout\n",
    "    function_approx.add(layers.Dense(20, activation = 'relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    function_approx.add(Dropout(0.5)) # Use 50% dropout\n",
    "    function_approx.add(layers.Dense(1, activation = 'linear'))\n",
    "    function_approx.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "    ## Define the callback list for early stopping   \n",
    "    filepath = 'my_model_file.hdf5' # define where the model checkpoints are saved\n",
    "    callbacks_list = [keras.callbacks.EarlyStopping( #monitor = 'val_loss', patience = 1)\n",
    "            monitor = 'val_loss', # Use loss to monitor the model\n",
    "            patience = 1 # Stop after one step with lower accuracy\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True # Only save model if it is the best\n",
    "        )\n",
    "    ]\n",
    "    return(function_approx)\n",
    "function_approx = DL_model()\n",
    "function_approx.predict(np.array([[0.1,0.1,0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 113\n",
      "Trainable params: 113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "function_approx.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below preforms greedy selection of the next action. The neural network is used to predict the action value for each possible action and the maximum is selected. Execute this code to run the simple test case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11847972, -1.0, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_Q(x,x_dot,actions,model):\n",
    "    Q = []\n",
    "    ## Iterate over all actions to find Q values\n",
    "    for i,a in enumerate(actions):\n",
    "        Q.append(model.predict(np.array([[x,x_dot,a]]))[0][0])\n",
    "    ## Find the action with max Q\n",
    "    max_index = np.argmax(Q)\n",
    "    return(Q[max_index], actions[max_index], max_index)\n",
    "        \n",
    "actions = [-1.0,0.0,1.0]        \n",
    "max_Q(0.1,0.0,actions,function_approx)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below execute a simple Q-Learning algorithm using the previously defined neural network model as the function approximator. Some important points for this algorithm are:\n",
    "1. An $\\epsilon$-greedy approach is used to select actions. \n",
    "2. The target action value,  $\\hat{q}(S_{t}, A_{t}, w_{t-1})$, used to train the neural network model is the 1-step bootstrapped reward estimate. \n",
    "\n",
    "Some additional details of the algorithm can be learned by reading the code comments. \n",
    "\n",
    "Execute this code for just 10 episodes and examine the results. This will take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "growing buffer\n",
      "[[1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "growing buffer\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]]\n",
      "\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def replay_buffer(new_cases, buffer, n_cases):\n",
    "    \"\"\"Function manages replay buffer as a numpy array with\n",
    "    maximum size of n_cases. Newest data is retained once\n",
    "    more than n_cases available\"\"\"\n",
    "    if(buffer.shape[0] <= n_cases): \n",
    "        print('growing buffer')\n",
    "        ## still growing buffer so append old cases to new\n",
    "        out = np.append(new_cases, buffer, axis = 0)\n",
    "    else: ## Delete some rows and add the new at the top\n",
    "        ## Create a mask for the rows we wish to keep or delete\n",
    "        del_rows = n_cases - new_cases.shape[0] + 1\n",
    "        mask = np.ones(buffer.shape[0], dtype=bool) \n",
    "        mask[del_rows:] = False\n",
    "        ## Append old cases to new \n",
    "        out = np.append(new_cases, buffer[mask,:], axis = 0)\n",
    "    return(out)\n",
    "\n",
    "b = np.zeros((3,2))\n",
    "add = np.ones((1,2))\n",
    "for _ in range(6):\n",
    "    b = replay_buffer(add, b, 4)\n",
    "    print(b)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_Q(episodes = 10, gamma = 0.9, epsilon = 0.05, alpha = 0.02, \n",
    "               n = 1, a_knot = 0.0, x_dot_knot = 0.0):\n",
    "    ## Possible actions\n",
    "    actions = [-1.0,0.0,1.0]\n",
    "    \n",
    "    ## Define the Keras model\n",
    "    function_approx = DL_model()\n",
    "\n",
    "    ## Loop over the episodes\n",
    "    for _ in range(episodes):\n",
    "        ## Initialize the car state\n",
    "        a = a_knot\n",
    "        x_dot = x_dot_knot\n",
    "        x = initalize_car()\n",
    "        x_list = []\n",
    "\n",
    "        done = False\n",
    "        i = 0 # Index for accumulting position and velocity steps\n",
    "        while(not done):\n",
    "            ## Update postion and velocity of the car given action and get the reward\n",
    "            x_prime, x_dot_prime, done, reward = sim_car(x, x_dot, a)\n",
    "            # Select and store the next action using the policy with epslion greedy approach\n",
    "            if(nr.uniform() > epsilon):\n",
    "                Q, a_prime, a_prime_index = max_Q(x_prime,x_dot_prime,actions,function_approx)\n",
    "            else: # Time to explore\n",
    "                a_prime_index = nr.choice(range(3)) #[0]\n",
    "                a_prime = actions[a_prime_index]\n",
    "                Q = function_approx.predict(np.array([[x_prime,x_dot_prime,a]]))[0][0]\n",
    "           \n",
    "            ## Compute the bootstrap estimate of Q. This s our target for re-fitting the\n",
    "            ## the NN regression model.\n",
    "            G = reward + gamma * function_approx.predict(np.array([[x_prime,x_dot_prime,a_prime]]))[0][0]\n",
    "            ## Finally, refit the model with the new target.\n",
    "            function_approx.fit(x = np.array([[x,x_dot,a]]), y = np.array([G]), epochs=1, verbose=0)\n",
    "            \n",
    "            ## Set action and state for next iteration\n",
    "            if(x_prime <= goal):\n",
    "                x = x_prime\n",
    "                x_dot = x_dot_prime\n",
    "                a = a_prime\n",
    "                i = i + 1\n",
    "                x_list.append(x)\n",
    "                \n",
    "        print('iterations = ' + str(i))\n",
    "    \n",
    "    return(function_approx)  \n",
    "\n",
    "deep_Q(episodes = 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations = 11312\n",
      "iterations = 2968\n",
      "iterations = 1296\n",
      "iterations = 130\n",
      "iterations = 544\n",
      "iterations = 1690\n"
     ]
    }
   ],
   "source": [
    "def deep_Q(episodes = 1000, gamma = 0.9, epsilon = 0.05, alpha = 0.02, \n",
    "               n = 1, goal = 0.5, a_knot = 0.0, x_dot_knot = 0.0):\n",
    "    ## Possible actions\n",
    "    actions = [-1.0,0.0,1.0]\n",
    "    \n",
    "    ## Define the Keras model\n",
    "    function_approx = DL_model()\n",
    "\n",
    "    ## Loop over the episodes\n",
    "    for _ in range(episodes):\n",
    "        ## Initialize the car state\n",
    "        a = a_knot\n",
    "        x_dot = x_dot_knot\n",
    "        x = initalize_car()\n",
    "        x_list = []\n",
    "\n",
    "        done = False\n",
    "        i = 0 # Index for accumulting position and velocity steps\n",
    "        while(not done):\n",
    "            ## Update postion and velocity of the car given action and get the reward\n",
    "            x_prime, x_dot_prime, done, reward = sim_car(x, x_dot, a)\n",
    "            # Select and store the next action using the policy with epslion greedy approach\n",
    "            if(nr.uniform() > epsilon):\n",
    "                Q, a_prime, a_prime_index = max_Q(x_prime,x_dot_prime,actions,function_approx)\n",
    "            else: # Time to explore\n",
    "                a_prime_index = nr.choice(range(3)) #[0]\n",
    "                a_prime = actions[a_prime_index]\n",
    "                Q = function_approx.predict(np.array([[x_prime,x_dot_prime,a]]))[0][0]\n",
    "           \n",
    "            ## Compute the bootstrap estimate of Q. This s our target for re-fitting the\n",
    "            ## the NN regression model.\n",
    "            G = reward + gamma * function_approx.predict(np.array([[x_prime,x_dot_prime,a_prime]]))[0][0]\n",
    "            ## Finally, refit the model with the new target.\n",
    "            function_approx.fit(x = np.array([[x,x_dot,a]]), y = np.array([G]), epochs=1, verbose=0)\n",
    "            \n",
    "            ## Set action and state for next iteration\n",
    "            if(x_prime <= goal):\n",
    "                x = x_prime\n",
    "                x_dot = x_dot_prime\n",
    "                a = a_prime\n",
    "                i = i + 1\n",
    "                x_list.append(x)\n",
    "                \n",
    "        print('iterations = ' + str(i))\n",
    "    \n",
    "    return(function_approx)  \n",
    "\n",
    "deep_Q(episodes = 10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of time steps required to reach the goal for each episode seems reasonable. However, as you have, no doubt noticed, this time required for even these 10 episodes is rather lengthy. In reality, thousands or tens of thousands of episodes are likely required to find a good approximation for the action values. This approximation would then be used to find a greedy policy given state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Off-Policy Function Approximation Algorithms\n",
    "\n",
    "In the foregoing example, we have used a rather naive approach to off-policy reinforcement learning with function approximation. Much better algorithms exist, including **eligibility trace** methods and **policy gradient** algorithms. Unfortunately, these algorithms are beyond the scope of this course. Additional information on these algorithms can be found in chapters 12 and 13 of Sutton and Barto, second edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, 2019, Stephen F. Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
