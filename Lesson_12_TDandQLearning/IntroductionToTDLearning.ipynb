{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TD and SARSA-Learning \n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "## Stephen Elston\n",
    "\n",
    "\n",
    "In the previous lesson we explored Monte Carlo **reinforcement learning**. MC RL required that the returns for an entire episode be computed before any values are available for use. The disadvantage of this approach is the the full set of returns are required for state value or action value estimates. But, how can we get state values or action-values in fewer time steps? It turns out there are algorithms which compute estimates in as few as one step. In this lesson will focus on a state-value estimation algorithm known as **time difference learning** or **TD-learning** and control algorithm known as **SARSA**. \n",
    "\n",
    "Recall that reinforcement learning has several distinctive characteristics, which differentiate this method from other machine learning and dynamic programming:\n",
    "- **No Markov model** needs to be specified for reinforcement learning, in contrast to dynamic programming.\n",
    "- Like dynamic programming, reinforcement learning **optimizes a reward function**. This is in contrast to supervised and unsupervised learning which use an error or objective function.  \n",
    "- Reinforcement learning algorithms learn by **experience**. Over time, the algorithm learns a model of the environment and these results are used to optimize the expected reward. Learning from experience is in contrast to supervised learning which uses known marked cases. \n",
    "- Reinforcement learning agents take **actions** and only receive **state** and **rewards** from the environment. These are the only interaction between the RL agent and the environment.    \n",
    "\n",
    "The interaction between a reinforcement learning agent and the environment are illustrated in the figure below. Notice that the only feedback the agent receives from the environment is reward and state.   \n",
    "\n",
    "<img src=\"img/RL_AgentModel.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Reinforcement Learning Agent and Environment** </center>  \n",
    "\n",
    "The ability to learn from experience is an attractive concept. This method of learning seems to mimic human learning. However, reinforcement learning has proven difficult to use in real-world applications. For a review of successes and problems arising when applying RL to robotics see [Kobler et. al.](https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf). At the present time, RL has mostly succeeded in cases where simulations can be used to gain experience. \n",
    "\n",
    "**Suggested readings** for TD and Q reinforcement learning, Chapters 6 and 7 of Sutton and Barto, second edition, provides a good introductions, including many alternative algorithms and details not discussed here.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Prediction Model\n",
    "\n",
    "In a previous lesson we examined a general update model:\n",
    "\n",
    "$$NewValue = OldValue + LearningRate * ErrorTerm$$\n",
    "\n",
    "Following this general formulation the update for TD state value can be written as:\n",
    "\n",
    "$$V(S_t) = V(S_t) + \\alpha \\big[ G_T - V(S_t) \\big]$$\n",
    "\n",
    "Here,   \n",
    "$\\alpha = $ the learning rate,    \n",
    "$\\big[ G_T - V(S_t) \\big] = $ the error term is the difference between the return $G_t$ and the value, $V(S_t)$. The return is identical to the value at convergence.   \n",
    "\n",
    "   \n",
    "Using the same general formulation we can create and single time step update model know as the **one step time difference** or **TD(0)** algorithm.\n",
    "\n",
    "$$V(S_t) = V(S_t) + \\alpha \\big[ R_{t+1} + V(S_{t+1}) - V(S_t) \\big]$$\n",
    "\n",
    "Where,  \n",
    "$\\delta_t =  R_{t+1} + V(S_{t+1}) - V(S_t) = $ the **TD error**,  \n",
    "$R_{t+1} = $ the return for the next time step,   \n",
    "$V(S_t) = $ is the value at time step t.   \n",
    "\n",
    "Like dynamic programming algorithms, the TD algorithm **bootstraps**. The return and estimated value at the next time step, $V(S_{t+1})$, are from previous samples. However, unlike MC RL which does not bootstrap, the TD(0) algorithm produces an estimate of $V(S_t)$ in only one time step, rather than waiting to reach the terminal state at the end of the episode. This fact can be seen by examining the backup diagram shown below:\n",
    "\n",
    "<img src=\"img/TD0.JPG\" alt=\"Drawing\" style=\"width:60px; height:150px\"/>\n",
    "<center> **Backup Diagram of TD(0)** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Time Difference RL\n",
    "\n",
    "With this short introduction TD RL in mind, let's try an example. We will sample the value function using a basic TD(0) algorithm here. \n",
    "\n",
    "As discussed in other labs, **Navigation** to a goal is a significant problem in robotics. Real-world navigation is rather complex. Therefore, in this example we will use a simple analog called a **grid world**. The grid world for this problem is shown below. \n",
    "\n",
    "<img src=\"img/GridWorld.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **A 4x4 Grid World with Terminal State** </center>\n",
    "\n",
    "The grid world consists of a 4x4 set of positions the robot can occupy. Each position is considered a state. The goal is to navigate to state 0, the goal, in the minimum steps. We will explore methods to find policies which reach this goal and achieve maximum reward. \n",
    "\n",
    "Grid position 0 is the goal and a **terminal state**. There are no possible state transitions out of this position. The presence of a terminal state makes this an **episodic Markov random process**. For each episode sampled the robot can start in any other random position, $\\{ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \\}$. This random selection process makes this a **random start** TD algorithm. The episode terminates when the robot enters the terminal position (state 0).  \n",
    "\n",
    "In reality, an RL agent may need to explore to find the possible actions when it is in some particular state. To simplify our example, we encode, or represent, these possibilities in a dictionary as shown in the code block below. We use a dictionary of dictionaries to perform the lookup. The keys of the outer dictionary are the identifiers (numbers) of the states. The keys of the inner dictionary are the possible actions and the values are the **successor state**, $s'$, for that transition.  \n",
    "\n",
    "In each state, there are four possible actions the robot can take:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "The TD RL agent has no model for the environment. Therefore, beyond these allowed actions, all other information is encapsulated in the environment and is unobservable by the agent. This is the key difference between reinforcement learning and dynamic programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "\n",
    "## Define the transition dictonary of dictionaries:\n",
    "neighbors = {0:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "          1:{'u':1, 'd':5, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':6, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':7, 'l':2, 'r':3},\n",
    "          4:{'u':0, 'd':8, 'l':4, 'r':5},\n",
    "          5:{'u':1, 'd':9, 'l':4, 'r':6},\n",
    "          6:{'u':2, 'd':10, 'l':5, 'r':7},\n",
    "          7:{'u':3, 'd':11, 'l':6, 'r':7},\n",
    "          8:{'u':4, 'd':12, 'l':8, 'r':9},\n",
    "          9:{'u':5, 'd':13, 'l':8, 'r':10},\n",
    "          10:{'u':6, 'd':14, 'l':9, 'r':11},\n",
    "          11:{'u':7, 'd':15, 'l':10, 'r':11},\n",
    "          12:{'u':8, 'd':12, 'l':12, 'r':13},\n",
    "          13:{'u':9, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':10, 'd':14, 'l':13, 'r':15},\n",
    "          15:{'u':11, 'd':15, 'l':14, 'r':15}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the environment, we need a reward structure. In this case, the robot receives the following rewards:   \n",
    "\n",
    "- 10 for entering position 0. \n",
    "- -1 for attempting to leave the grid. In other words, we penalize the robot for hitting the edges of the grid.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another. If we did not have this penalty, the robot could follow any random plan to the goal which did not hit the edges. \n",
    "\n",
    "This **reward structure is unknown to the TD RL agent**. The agent must **learn** the rewards by sampling the environment. Here the rewards are in the form of action values.    \n",
    "\n",
    "We encode these rewards in the same type of dictionary structure used for the foregoing structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {0:{'u':10.0, 'd':10.0, 'l':10.0, 'r':10.0},\n",
    "          1:{'u':-1, 'd':-0.1, 'l':10.0, 'r':-0.1},\n",
    "          2:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          4:{'u':10.0, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          6:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          7:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          8:{'u':-0.1, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          11:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          12:{'u':-0.1, 'd':-1.0, 'l':-1.0, 'r':-0.1},\n",
    "          13:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          15:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-1.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was done previously, we will use an environment simulator. The function is called with a state and action. It returns the next state, 's_prime' and 'reward'. \n",
    "\n",
    "To simplify the rest of the code in this notebook we are treating the dictionaries as global. In general, this would be considered poor programming practice. \n",
    "\n",
    "Execute the code in the cell below and observe the results from the test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, -1, False)\n",
      "(5, -0.1, False)\n",
      "(2, -0.1, False)\n",
      "(0, 10.0, True)\n"
     ]
    }
   ],
   "source": [
    "def simulate_environment(s, action, neighbors = neighbors, rewards = rewards, terminal = 0):\n",
    "    \"\"\"\n",
    "    Function simulates the environment\n",
    "    returns s_prime and reward given s and action\n",
    "    \"\"\"\n",
    "    s_prime = neighbors[s][action]\n",
    "    reward = rewards[s][action]\n",
    "    return (s_prime, reward, is_terminal(s_prime, terminal))\n",
    "\n",
    "def is_terminal(state, terminal = 0):\n",
    "    return state == terminal\n",
    "\n",
    "## Test the function\n",
    "for a in ['u', 'd', 'r', 'l']:\n",
    "    print(simulate_environment(1, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) Policy Evaluation\n",
    "\n",
    "We have everything in place to perform TD(0) policy evaluation for the grid world. The code in the cell below implements the TD(0) algorithm and applies it to the policy in the grid world. Notice that the algorithm makes calls to the aforementioned `state_values` function to find information on successor state and rewards for a transition given a current state and action. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_policy = {0:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                        2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 3, 14, 14, 5, 7, 11, 10, 11, 6]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def start_episode(n_states):\n",
    "    '''Function to find a random starting value for the episode\n",
    "    that is not the terminal state'''\n",
    "    state = nr.choice(range(n_states))\n",
    "    while(is_terminal(state)):\n",
    "         state = nr.choice(range(n_states))\n",
    "    return state\n",
    "\n",
    "## test the function to make sure never starting in terminal state\n",
    "[start_episode(15) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('r', 0, 10.0, True)\n",
      "('u', 1, -1, False)\n",
      "('l', 1, -0.1, False)\n",
      "('u', 3, -1.0, False)\n",
      "('u', 0, 10.0, True)\n",
      "('d', 9, -0.1, False)\n",
      "('d', 10, -0.1, False)\n",
      "('d', 11, -0.1, False)\n",
      "('d', 12, -0.1, False)\n",
      "('d', 13, -0.1, False)\n",
      "('d', 14, -0.1, False)\n",
      "('d', 15, -0.1, False)\n",
      "('u', 8, -0.1, False)\n",
      "('r', 14, -0.1, False)\n",
      "('d', 14, -1.0, False)\n",
      "('l', 14, -0.1, False)\n"
     ]
    }
   ],
   "source": [
    "def take_action(state, policy, actions = {1:'u', 2:'d', 3:'l', 4:'r'}):\n",
    "    '''Function takes action given state using the transition probabilities \n",
    "    of the policy'''\n",
    "    ## Find the action given the transistion probabilities defined by the policy.\n",
    "    action = actions[nr.choice(range(len(actions)), p = list(policy[state].values())) + 1]\n",
    "    s_prime, reward, terminal = simulate_environment(state, action)\n",
    "    return (action, s_prime, reward, terminal)\n",
    "\n",
    "## Test function for several states\n",
    "for s in range(16):\n",
    "    print(take_action(s, initial_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  5.96427383, -0.4068003 , -3.67199232],\n",
       "       [ 0.62305303, -1.06508283, -1.65603412, -4.61449623],\n",
       "       [-3.11360546, -2.93968136, -3.73579961, -4.58630378],\n",
       "       [-4.5223916 , -4.29647326, -4.93544762, -5.10285281]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def td_0_state_values(policy, n_samps, alpha = 0.2, gamma = 1.0):\n",
    "    \"\"\"\n",
    "    Function for TD(0) policy evalutation\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Find the starting state\n",
    "    n_states = len(policy)\n",
    "    current_state = start_episode(n_states)\n",
    "    terminal = False\n",
    "    \n",
    "    ## Array for state values\n",
    "    v = np.zeros((n_states,1))\n",
    "    \n",
    "    for _ in range(n_samps):\n",
    "        ## Find the next action and reward\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "        ## Compute the TD error\n",
    "        delta = reward + gamma*v[s_prime] - v[current_state]\n",
    "        ## Update the state value\n",
    "        v[current_state] = v[current_state] + alpha*delta\n",
    "        current_state = s_prime\n",
    "        if(terminal): ## start new episode when terminal\n",
    "            current_state = start_episode(n_states)\n",
    "    return(v)\n",
    "\n",
    "td_0_state_values(initial_policy, 2000).reshape((4,4))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Step SARSA Algorithm\n",
    "\n",
    "Now that we have examined the one step TD(0) algorithm for policy (value) evaluation, we need to define a one step algorithm for **control** or **policy improvement**.     \n",
    "\n",
    "The **state action reward state action** or **SARSA** algorithm is an **on policy** method which uses time differencing to evaluate action values. As you might imagine from the name, this algorithm starts with an **on policy** action from the current state which results in a state transition and a reward. The backup diagram for one step SARSA (SARSA(0)) is shown in the figure below. \n",
    "\n",
    "<img src=\"img/SARSA.JPG\" alt=\"Drawing\" style=\"width:75px; height:150px\"/>\n",
    "<center> **Backup Diagram for SARSA(0)** </center>\n",
    "\n",
    "Compare the backup diagram for SARSA(0) to the one for TD(0) notice that the order of state and action are reversed between the two algorithms. This realization is a good way to understand the difference. \n",
    "\n",
    "The update equation for SARSA(0) is as follows:\n",
    "\n",
    "$$Q(S_t,A_t) = Q(S_t,A_t) + \\alpha \\big[ R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) \\big]$$  \n",
    "\n",
    "Where,   \n",
    "$\\delta_t = R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) = $ the error term,   \n",
    "$Q(S_t,A_t) = $ is the action value in state S given action A,  \n",
    "$R_{t+1} = $ is the reward for the next time step,   \n",
    "$\\alpha = $ the learning rate,   \n",
    "$\\gamma = $ discount factor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA(0) Example\n",
    "\n",
    "The code in the cell below implements the SARSA(0) algorithm to compute the action values for the Grid world given an policy. As with the policy evaluation code, this code makes calls to the environment t find successor states and rewards given actions. The function `select_a_prime` queries the environment to receive the successor state and reward given the current state and action. The successor action is computed following the policy. Additional details on this algorithm can be seen by reading the code comments.  \n",
    "\n",
    "Execute this code to compute and print the action values for the random walk policy.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          up       down       left      right\n",
      "0   0.000000   0.000000   0.000000   0.000000\n",
      "1  -4.288728  -5.979460  -1.031388  -8.047501\n",
      "2  -7.619536  -8.356775  -4.352879  -9.413155\n",
      "3  -9.952437  -9.285981  -7.193611 -10.266075\n",
      "4   0.413013  -7.077807  -4.792448  -6.749730\n",
      "5  -4.670843  -7.866950  -5.001349  -8.445354\n",
      "6  -8.180762  -8.848110  -6.783869  -9.449058\n",
      "7  -8.866155 -10.022287  -7.134412 -10.182924\n",
      "8  -4.228575  -9.012395  -9.009645  -7.590222\n",
      "9  -6.061991  -9.609473  -6.886565  -8.455171\n",
      "10 -8.098456  -9.996591  -8.092602  -9.779688\n",
      "11 -8.749750 -10.532404  -7.999666 -11.236206\n",
      "12 -7.703450  -9.837287 -10.069488  -9.089910\n",
      "13 -8.106208  -9.577142  -8.485450  -9.975737\n",
      "14 -8.791056 -10.587347  -9.363335 -10.403248\n",
      "15 -9.384820 -10.955134  -9.735906 -10.805682\n"
     ]
    }
   ],
   "source": [
    "def print_Q(Q):\n",
    "    Q = pd.DataFrame(Q, columns = ['up', 'down', 'left', 'right'])\n",
    "    print(Q)\n",
    "\n",
    "def new_episode(n_states, policy):\n",
    "    '''This function provides a start for a TD\n",
    "    episode making sure the first transition is not \n",
    "    the termnal state'''\n",
    "    current_state = start_episode(n_states)\n",
    "    ## Find fist action and reward\n",
    "    action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "    return(current_state, action, s_prime, reward, terminal)    \n",
    "\n",
    "\n",
    "def SARSA_0(policy, n_samps, alpha = 0.1, gamma = 0.9, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    \"\"\"\n",
    "    Function for TD(0) policy evalutation\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Find the starting state\n",
    "    n_states = len(policy)\n",
    "    current_state, action, s_prime, reward, terminal = new_episode(n_states, policy)\n",
    "    action_idx = action_index[action]\n",
    "    \n",
    "    ## Array for state values\n",
    "    q = np.zeros((n_states, len(policy[0])))\n",
    "    \n",
    "    for _ in range(n_samps):\n",
    "        ## Find the next action and reward\n",
    "        action_prime, s_prime_prime, reward_prime, terminal_prime = take_action(s_prime, policy)\n",
    "        action_idx_prime = action_index[action_prime]\n",
    "        ## Compute the TD error\n",
    "        delta = reward + gamma*q[s_prime, action_idx_prime] - q[current_state, action_idx]\n",
    "        ## Update the action values\n",
    "        q[current_state, action_idx] = q[current_state, action_idx] + alpha*delta\n",
    "        ## Update the state, action and reward for the next time step\n",
    "        current_state = s_prime\n",
    "        s_prime = s_prime_prime\n",
    "        action = action_prime\n",
    "        reward = reward_prime\n",
    "        terminal = terminal_prime\n",
    "        action_idx = action_idx_prime\n",
    "\n",
    "        ## Check if end of episode\n",
    "        if(terminal): \n",
    "            ## start new episode\n",
    "            current_state, action, s_prime, reward, terminal = new_episode(n_states, policy)        \n",
    "    return(q)\n",
    "\n",
    "\n",
    "Q = SARSA_0(initial_policy, 20000, alpha = 0.2, gamma = 0.99)\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays printed above display the action values for each state. The four arrays represent the four possible actions, up, down, left and right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPI with SARSA(0)\n",
    "\n",
    "The code in the cell below performs **general policy improvement (GPI)** using SARSA(0) to evaluate action values. Several cycles of SARSA(0) and policy improvement are performed with this code. Notice that policy improvement is $\\epsilon$-greedy. The probability of any state transition will never go to zero, allowing continued exploration. Additional details on this algorithm can be seen by reading the code comments.  \n",
    "\n",
    "Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 1: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 2: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 3: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 4: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 5: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 6: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 7: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 8: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 9: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 10: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 11: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 12: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 13: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 14: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 15: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_policy(policy, Q, epsilon, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    '''Updates the policy based on estiamtes of Q using \n",
    "    an epslion greedy algorithm. The action with the highest\n",
    "    action value is used.'''\n",
    "    \n",
    "    ## Find the keys for the actions in the policy\n",
    "    keys = list(policy[0].keys())\n",
    "    \n",
    "    ## Iterate over the states and find the maximm action value.\n",
    "    for state in range(len(policy)):\n",
    "        ## First find the index of the max Q values  \n",
    "        q = Q[state,:]\n",
    "        max_action_index = np.where(q == max(q))[0]\n",
    "\n",
    "        ## Find the probabilities for the transitions\n",
    "        n_transitions = float(len(q))\n",
    "        n_max_transitions = float(len(max_action_index))\n",
    "        p_max_transitions = (1.0 - epsilon *(n_transitions - n_max_transitions))/(n_max_transitions)\n",
    "  \n",
    "        ## Now assign the probabilities to the policy as epsilon greedy.\n",
    "        for key in keys:\n",
    "            if(action_index[key] in max_action_index): policy[state][key] = p_max_transitions\n",
    "            else: policy[state][key] = epsilon\n",
    "    return(policy)                \n",
    "\n",
    "update_policy(initial_policy, Q, 0.1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.3, 'd': 0.1, 'l': 0.3, 'r': 0.3}\n",
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.3, 'd': 0.1, 'l': 0.3, 'r': 0.3}\n",
      "{'u': 0.4, 'd': 0.1, 'l': 0.4, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n"
     ]
    }
   ],
   "source": [
    "def SARSA_GPI(policy, n_samples, n_cycles, epsilon = 0.1, n_actions = 4):\n",
    "    '''Function perfoms GPI using Monte Carlo value estimation.\n",
    "    Updates to policy are epsilon greedy to prevent the algorithm\n",
    "    from being trapped at some point.'''\n",
    "    Q = np.zeros((len(policy), n_actions))\n",
    "    ## Iterate over the required number of cycles\n",
    "    for _ in range(n_cycles):\n",
    "        Q = SARSA_0(policy, n_samples, alpha = 0.2, gamma = 0.99)\n",
    "        policy = update_policy(policy, Q, epsilon = epsilon)\n",
    "    return(policy)\n",
    "\n",
    "improved_policy = SARSA_GPI(initial_policy, 100, 50, epsilon = 0.1)  \n",
    "for state in range(16):\n",
    "    print(improved_policy[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Step Time Differencing\n",
    "\n",
    "In the previous lesson we examined Monte Carlo RL which requires that each episode reach a terminal state before a value or action value estimate can be made. In the first part of this lesson we have focused on one-step time differencing, TD, algorithms which update estimates at each time step. Now, we will look at algorithms between these two extreme cases, n-step time differencing algorithms. \n",
    "\n",
    "We can generalize the concept of the one-step bootstrapping of the return starting with TD(0):\n",
    "\n",
    "$$G_{t:t+1} = R_{t+1} + \\gamma V_{t}(S_{t+1})$$\n",
    "\n",
    "Here, $V_{t}(S_{t+1})$ is being bootstrapped. \n",
    "\n",
    "We can extend this formulation to two-step TD as follows:\n",
    "\n",
    "$$G_{t:t+2} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V_{t+1}(S_{t+2})$$\n",
    "\n",
    "In the above it is $V_{t+1}(S_{t+2})$ being bootstrapped. \n",
    "\n",
    "For n-step TD the return is computed by the following bootstrap formulation. \n",
    "\n",
    "$$G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n})$$\n",
    "\n",
    "Regardless of home many steps are used to compute the return, the value update is:\n",
    "\n",
    "$$V_{t+n}(S_t) = V_{t+n-1}(S_t) + \\alpha \\big[ G_{t:t+n} - V_{t+n-1}(S_t) \\big],\\ 0 \\leq t < T]$$\n",
    "\n",
    "Where $\\delta_t = G_{t:t+n} - V_{t+n-1}(S_t) = $ the n-step TD error.\n",
    "\n",
    "The resulting backup diagrams for general TD(n) is shown in the diagram below. \n",
    "\n",
    "<img src=\"img/TDN.JPG\" alt=\"Drawing\" style=\"width:400px; height:350px\"/>\n",
    "<center> **Backup Diagram for n-step TD** </center>\n",
    "\n",
    "From Left to right you can see how the backup diagram adds more states and actions. On the extreme right the backup diagram for the MC algorithm is shown. Notice that MC RL does not bootstrap, but samples until the terminal state is reached. \n",
    "\n",
    "N-step TD methods are generally considered to have two advantages over single-step methods:\n",
    "1. N-step methods generally have better convergence properties. \n",
    "2. N-step methods break the link between time between actions and time when bootstraping occurs.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of TD(n) for Policy Evaluation\n",
    "\n",
    "The code in the cell below implements TD(n) policy evaluation. As you can see, the bookkeeping is a bit involved. We must account for the cases where fewer than n steps have been executed at the start of an episode or when the terminal state has been reached. You can find further details of the algorithm by reading the code comments. \n",
    "\n",
    "Execute this code for n=4 and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.22871027, -0.02102871, -0.0521305 ],\n",
       "       [ 0.2052914 ,  0.19764216,  0.06445226,  0.04238113],\n",
       "       [-0.20706724,  0.083657  ,  0.0510555 , -0.31084057],\n",
       "       [-0.3820496 , -0.27003828,  0.0384893 , -0.11496626]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TD_n(policy, episodes, n, alpha = 0.2, gamma = 0.9, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    Function to perform TD(N) policy evaluation.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    v = np.zeros((n_states))\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        ## Initialize variables\n",
    "        T = float(\"inf\")\n",
    "        tau = 0\n",
    "        t = 0\n",
    "        rewards = []\n",
    "       \n",
    "        ## Get the random initial state\n",
    "        current_state = start_episode(n_states)   \n",
    "        state = [current_state]\n",
    "        ## Initial action\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "        state.append(s_prime)\n",
    "        \n",
    "        while(not (tau == T - 1)):\n",
    "#        while(not terminal):\n",
    "#            print('tau = {}    T = {}      t = {}    terminal = {}'.format(tau,T,t,terminal))\n",
    "            if(t < T):\n",
    "                ## Append the reward to the list\n",
    "                rewards.append(reward)\n",
    "                ## Get the next action state and rewards\n",
    "                action_prime, s_prime_prime, reward_prime, terminal_prime = take_action(current_state, policy)\n",
    "                state.append(s_prime_prime)\n",
    "                ## update T if at terminal state\n",
    "                if(terminal): T = t + 1\n",
    "                    \n",
    "            ## Update tau\n",
    "            tau = t - n + 1\n",
    "            \n",
    "            if(tau > 0):\n",
    "                G = 0.0\n",
    "                for i in range(tau + 1, min(tau + n, T)):\n",
    "                    exponent = i + tau - 1\n",
    "                    G = G + gamma**exponent * rewards[i]\n",
    "                if(tau + n < T): G += gamma**n * v[state[tau + n]] \n",
    "                v[state[tau]] = v[state[tau]] + alpha * (G - v[state[tau]])    \n",
    "                \n",
    "            \n",
    "            ## Update variables for the next step\n",
    "            t += 1\n",
    "            action = action_prime\n",
    "            s_prime = s_prime_prime\n",
    "            current_state = s_prime\n",
    "            reward = reward_prime\n",
    "            terminal = terminal_prime\n",
    "    return(v)              \n",
    "            \n",
    "TD_n(initial_policy, 2000, 4).reshape((4,4))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-step SARSA for control\n",
    "\n",
    "Just as we used SARSA(0) for the control or policy improvement, we can generalize this algorithm to the n-step case. This is the SARSA(n) algorithm.  \n",
    "\n",
    "The n-step return is computed by the following bootstrap formulation. \n",
    "\n",
    "$$G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}),\\ n \\geq 1, 0 \\leq t < T-n$$\n",
    "\n",
    "Using this return, the action value update becomes:\n",
    "\n",
    "$$Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \\alpha \\big[ G_{t:t+n} - Q_{t+n-1}(S_t,A_t) \\big],\\ 0 \\leq t < T]$$\n",
    "\n",
    "Where $\\delta_t =  G_{t:t+n} - Q_{t+n-1}(S_t,A_t) = $ the n-step error.\n",
    "\n",
    "Backup diagrams for n-step SARSA are shown in the figure below. These backups reverse the order of state and action when compared to the n-step TD prediction algorithm discussed above. \n",
    "\n",
    "<img src=\"img/SARSAN.JPG\" alt=\"Drawing\" style=\"width:400px; height:350px\"/>\n",
    "<center> **Backup Diagram for n-step SARSA** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of N-Step SARSA\n",
    "\n",
    "The code in the cell below implements the n-step SARSA algorithm. As is the case for the n-step TD algorithm, the bookkeeping is a bit involved. We must account for the cases where fewer than n steps have been executed at the start of an episode or when the terminal state has been reached. You can find further details of the algorithm by reading the code comments.\n",
    "\n",
    "Execute this code for n=4 and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          up      down      left     right\n",
      "0   0.000000  0.000000  0.000000  0.000000\n",
      "1   0.207048  0.625418  3.328222  0.251014\n",
      "2   0.705346  1.326987  2.461137  0.607597\n",
      "3   0.437248  0.145293  1.980233 -0.010253\n",
      "4   4.018617  0.395191  0.497663  0.196402\n",
      "5   1.940484  1.278724  1.801030  0.751198\n",
      "6   0.124051  0.284372  0.898855  0.370027\n",
      "7   0.644459  0.194000  1.118285  0.363199\n",
      "8   3.083102  0.745815  0.857989  0.718671\n",
      "9   0.679412  0.117494  1.998091  0.221445\n",
      "10  1.019813  0.270148  1.403928  0.474835\n",
      "11  0.114455 -0.253603  0.287913 -0.491576\n",
      "12  1.480690  0.378191  0.691628 -0.021194\n",
      "13  1.798119  0.297119  1.988624  0.256945\n",
      "14  0.151144 -0.185391  1.647604 -0.365426\n",
      "15 -0.063934 -0.298126  0.394381 -0.662435\n"
     ]
    }
   ],
   "source": [
    "def SARSA_n(policy, episodes, n, alpha = 0.2, gamma = 0.9, epsilon = 0.1, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    \"\"\"\n",
    "    Function to perform TD(N) policy evaluation.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "#    action_index = list(range(len(list(policy[0].keys()))))\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    n_actions = len(policy[0].keys())\n",
    "    q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        ## Initialize variables\n",
    "        T = float(\"inf\")\n",
    "        tau = 0\n",
    "        t = 0\n",
    "        rewards = []\n",
    "       \n",
    "        ## Get the random initial state\n",
    "        current_state = start_episode(n_states)   \n",
    "        state = [current_state]\n",
    "        ## Initial action\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "        state.append(s_prime)\n",
    "        \n",
    "        while(not (tau == T - 1)):\n",
    "#        while(not terminal):\n",
    "#            print('tau = {}    T = {}      t = {}    terminal = {}'.format(tau,T,t,terminal))\n",
    "            if(t < T):\n",
    "                ## Append the reward to the list\n",
    "                rewards.append(reward)\n",
    "                ## update T if at terminal state\n",
    "                if(terminal): T = t + 1\n",
    "                else: \n",
    "                    ## Get the next action state and rewards\n",
    "                    action_prime, s_prime_prime, reward_prime, terminal_prime = take_action(current_state, policy)\n",
    "                    state.append(s_prime_prime)\n",
    "                    \n",
    "                    \n",
    "            ## Update tau\n",
    "            tau = t - n + 1\n",
    "            \n",
    "            if(tau > 0):\n",
    "                G = 0.0\n",
    "                for i in range(tau + 1, min(tau + n, T)):\n",
    "                    exponent = i + tau - 1\n",
    "                    G = G + gamma**exponent * rewards[i]\n",
    "                if(tau + n < T): G += gamma**n * q[state[tau + n], action_index[action_prime]] \n",
    "                q[state[tau], action_index[action]] = q[state[tau], action_index[action]] + alpha * (G - q[state[tau], action_index[action]])    \n",
    "                \n",
    "            \n",
    "            ## Update variables for the next step\n",
    "            t += 1\n",
    "            current_state = s_prime\n",
    "            if(not terminal):\n",
    "                action = action_prime\n",
    "                s_prime = s_prime_prime\n",
    "                reward = reward_prime\n",
    "                terminal = terminal_prime\n",
    "    return(q)              \n",
    "            \n",
    "Q = SARSA_n(initial_policy, 1000, 4)\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n"
     ]
    }
   ],
   "source": [
    "improved_policy = update_policy(initial_policy, Q, 0.1) \n",
    "for state in range(16):\n",
    "    print(improved_policy[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.42539325, 3.25330626, 3.01848003],\n",
       "       [0.22010317, 3.66039451, 2.43586767, 1.62419453],\n",
       "       [4.71031975, 3.05326416, 0.97939887, 0.96392466],\n",
       "       [1.29324221, 1.72951677, 1.44113193, 1.14311417]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD_n(improved_policy, 2000, 4).reshape((4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPI with N-step SARSA\n",
    "\n",
    "The code in the cell below executes the GPI algorithm using n-step SARSA to evaluate the action values. Details of the algorithm can be found by reading the code comments. \n",
    "\n",
    "Execute the code using 50 cycles of 100 episodes and 4-step SARSA and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def SARSA_n_GPI(policy, n, cycles, episodes, goal, alpha = 0.2, gamma = 0.9, epsilon = 0.1):\n",
    "    ## iterate over GPI cycles\n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    for _ in range(cycles):\n",
    "        ## Evaluate policy with SARSA\n",
    "        Q = SARSA_n(policy, episodes, n, goal = goal, alpha = alpha, gamma = gamma, epsilon = epsilon)\n",
    "        \n",
    "        for s in list(current_policy.keys()): # iterate over all states\n",
    "            ## Find the index action with the largest Q values \n",
    "            ## May be more than one. \n",
    "            max_index = np.where(Q[:,s] == max(Q[:,s]))[0]\n",
    "            \n",
    "            ## Probabilities of transition\n",
    "            ## Need to allow for further exploration so don't let any \n",
    "            ## transition probability be 0.\n",
    "            ## Some gymnastics are required to ensure that the probabilities \n",
    "            ## over the transistions actual add to exactly 1.0\n",
    "            neighbors_len = float(Q.shape[0])\n",
    "            max_len = float(len(max_index))\n",
    "            diff = round(neighbors_len - max_len,3)\n",
    "            prob_for_policy = round(1.0/max_len,3)\n",
    "            adjust = round((epsilon * (diff)), 3)\n",
    "            prob_for_policy = prob_for_policy - adjust\n",
    "            if(diff != 0.0):\n",
    "                remainder = (1.0 - max_len * prob_for_policy)/diff\n",
    "            else:\n",
    "                remainder = epsilon\n",
    "                                                 \n",
    "            for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "                if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "                else: current_policy[s][key] = remainder   \n",
    "                    \n",
    "    return(current_policy)                    \n",
    " \n",
    "\n",
    "SARSA_N_Policy = SARSA_n_GPI(policy, n = 4, cycles = 5, episodes = 1000, goal = 0, alpha = 0.2, epsilon = 0.1)\n",
    "SARSA_N_Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, execute the code in the cell below to evaluate the policy you have just computed using the n-step TD algorithm with n = 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(TD_n(SARSA_N_Policy, episodes = 1000, n = 4, goal = 0, alpha = 0.2, gamma = 0.9)).reshape((4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Step Off-Policy Learning with Importance Sampling\n",
    "\n",
    "For n-step off-policy learning we update a target policy $\\pi(A_t|S_t)$ using samples from a behavior policy $b(A_t|S_t)$. Since the two policies differ, the probabilities of an action given the state will undoubtedly differ. For example, the behavior policy can be exploratory whereas, the target policy is greedy. \n",
    "\n",
    "To account for the different probabilities of sampling we reweight by the **importance sampling ratio**. For an n-step algorithm at time step $t$ the importance sampling ratio can be expressed as:\n",
    "\n",
    "$$\\rho_{t:t + n -1} = \\prod_{k=\\tau}^{min(t + n -1,T-1)} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}$$\n",
    "\n",
    "The n-step TD update then becomes:\n",
    "\n",
    "$$V_{t+n}(S_t) = V_{t+n-1}(S_t) + \\alpha\\ \\rho_{t:t+n-1} \\big[ G_{t:t+n} - V_{t+n-1}(S_t) \\big],\\ 0 \\leq t < T]$$\n",
    "\n",
    "And the SARSA update becomes:\n",
    "\n",
    "$$Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \\alpha\\ \\rho_{t:t+n-1} \\big[ G_{t:t+n} - Q_{t+n-1}(S_t,A_t) \\big],\\ 0 \\leq t < T]$$\n",
    "\n",
    "For both of the above update equations consider the effect of importance sampling ratio. If the action given state is more likely under the target policy that the behavior policy, more weight is given to updating with the error term. However, If the action given state is less likely under the target policy that the behavior policy, less weight is given to updating with the error term. In this way, the weighting by the importance sampling ratio gives the correct updates for the target policy regardless of the transition probabilities of the behavior policy. \n",
    "\n",
    "> **NOte:** Considerably more detail on n-step off-policy RL algorithms can be found in Sutton and Barto, second edition, Sections 7.3, 7.4 and 7.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
